#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.12"
# dependencies = [
#   "transformers",
#   "requests",
#   "rich",
#   "torch",
#   "nltk",
#   "html2text",
# ]
# ///
"""Prune

Takes as input some content, and a prompt, and 
runs a model which reduces the content down to only that
which is essential for the prompt. This is a prompt optimization
strategy which could potentially reduce token usage by quite
a significant amount.

"""

from transformers import AutoModel
import os
import requests
from rich.console import Console
from rich.markdown import Markdown
from rich.table import Table
import nltk
import html2text
import time
from contextlib import contextmanager
import torch

timings = dict()


@contextmanager
def timer(name: str):
    """Context manager to time code blocks."""
    start = time.perf_counter()
    yield
    elapsed = time.perf_counter() - start
    if not timings.get(name):
      timings[name] = 0
    timings[name] += elapsed


try:
    nltk.data.find("tokenizers/punkt_tab")
except LookupError:
    nltk.download("punkt_tab")


def prune(content: str, question: str, model: str):
    # Check for GPU availability
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    with timer("Model Loading"):
        provence = AutoModel.from_pretrained(model, trust_remote_code=True)
        provence = provence.to(device)

    with timer("Model Processing"):
        result = provence.process(question, content, enable_warnings=False)
    
    return result, device


def get_text_from_input(input_str: str) -> str:
    """Load content from input."""
    # Check if it is a path to an existing file
    if os.path.isfile(input_str):
        with timer("File Reading"):
            with open(input_str, "r", encoding="utf-8") as f:
                return f.read()

    # Check if it looks like a URL
    if input_str.startswith(("http://", "https://")):
        try:
            with timer("URL Fetching"):
                response = requests.get(input_str)
                response.raise_for_status()
            with timer("HTML Conversion"):
                content = html2text.html2text(response.text)
            return content
        except requests.RequestException as e:
            raise ValueError(f"Failed to fetch URL '{input_str}': {e}")

    return input_str


if __name__ == "__main__":
    import argparse
    console = Console()

    with timer("Total Runtime"):
        parser = argparse.ArgumentParser(
            description="Prune provided content to only contain prompt relevant information."
        )
        parser.add_argument(
            "-i",
            "--src",
            help="Input source as either a file, string, or url that should be pruned.",
        )
        parser.add_argument(
            "-p", "--prompt", help="Question/prompt used for pruning the content."
        )
        parser.add_argument(
            "-m",
            "--model",
            default="naver/provence-reranker-debertav3-v1",
            help="Pruning model",
        )
        parser.add_argument(
          "--profiling",
          action="store_true",
          help="Flag used for profiling the script"
        )
        

        args = parser.parse_args()

        content = get_text_from_input(args.src)
        pruned_content, device = prune(content, args.prompt, args.model)
        console.print(pruned_content["pruned_context"])

    # Display timing results
    if args.profiling:
        console.print("\n[bold cyan]Performance Profiling Results:[/bold cyan]")
        console.print(f"[bold]Device Used:[/bold] {device.upper()}")
        if device == "cuda":
            gpu_name = torch.cuda.get_device_name(0)
            console.print(f"[bold]GPU:[/bold] {gpu_name}\n")
        else:
            console.print("[yellow]Note: No GPU detected, using CPU (consider installing CUDA for faster processing)[/yellow]\n")
        
        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("Operation", style="cyan", no_wrap=True)
        table.add_column("Time (seconds)", justify="right", style="green")
        table.add_column("Percentage", justify="right", style="yellow")
        
        total_time = timings.get("Total Runtime", sum(timings.values()))
        
        # Sort by time descending (excluding total)
        sorted_timings = sorted(
            [(k, v) for k, v in timings.items() if k != "Total Runtime"],
            key=lambda x: x[1],
            reverse=True
        )
        
        for operation, elapsed in sorted_timings:
            percentage = (elapsed / total_time) * 100
            table.add_row(operation, f"{elapsed:.4f}", f"{percentage:.2f}%")
        
        table.add_row("─" * 20, "─" * 15, "─" * 15, style="dim")
        table.add_row("[bold]Total Runtime[/bold]", f"[bold]{total_time:.4f}[/bold]", "[bold]100.00%[/bold]")
        
        console.print(table)
